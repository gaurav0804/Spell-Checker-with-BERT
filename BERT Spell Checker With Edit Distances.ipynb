{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import textdistance\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text.(Do not forget to place punctuation): spelling correction is quote diffcult task !\n"
     ]
    }
   ],
   "source": [
    "# Input single sentence\n",
    "text = input('Enter text.(Do not forget to place punctuation): ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'spelling',\n",
       " 'correction',\n",
       " 'is',\n",
       " 'quote',\n",
       " 'diffcult',\n",
       " 'task',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Break down sentence in to tokens and add special tokens to indicate begining and end of sentence\n",
    "text = WordPunctTokenizer().tokenize(text)\n",
    "text.insert(0,'[CLS]')\n",
    "text.insert(len(text),'[SEP]')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spelling\n",
      "{'spell': 8.29263444757089e-05, 'spelling': 7.319104042835534e-05, 'Feeling': 3.1823601602809504e-05, 'Opening': 2.290070551680401e-05, 'something': 1.0126244887942448e-05}\n",
      "correction\n",
      "{'correction': 0.010122857056558132, 'convention': 0.001368048251606524, 'competition': 0.0011327865067869425, 'combination': 0.0008300655172206461, 'direction': 0.0006971591501496732}\n",
      "is\n",
      "{',': 0.2501995265483856, '.': 0.12372457981109619, '!': 0.06161179766058922, ';': 0.05436330288648605, '-': 0.05090939253568649}\n",
      "quote\n",
      "{'quite': 4.846660885959864e-06, 'quit': 7.05156750768765e-08, 'wrote': 5.465818730954197e-08, 'que': 5.041277262307631e-08, 'mute': 2.6868049118888848e-08}\n",
      "diffcult\n",
      "{'difficult': 0.002158178947865963, 'different': 0.00029643948073498905, 'difficulty': 4.1997889638878405e-05, 'default': 1.6654354112688452e-05, 'difficulties': 1.1171185178682208e-05}\n",
      "task\n",
      "{'talk': 6.015098188072443e-05, 'task': 2.4907898477977142e-05, 'track': 1.802486258384306e-05, 'taste': 1.5805690054548904e-05, 'talks': 1.1343165169819258e-05}\n",
      "!\n",
      "{'.': 0.7819957733154297, ';': 0.21720373630523682, '!': 0.0002417742507532239, ',': 0.00014730662223882973, '?': 0.00014450994785875082}\n"
     ]
    }
   ],
   "source": [
    "#Iterate over every token in the sequence and replace it with special token [MASK]. The model will then try to predict the token\n",
    "#in the [MASK] position\n",
    "for i,token in enumerate(text):\n",
    "    copy_text = text[:]  \n",
    "    \n",
    "    if token not in ('[CLS]','[SEP]'):\n",
    "        print(copy_text[i])\n",
    "        original_token = copy_text[i]\n",
    "        copy_text[i]='[MASK]'\n",
    "        copy_text = ' '.join(copy_text)\n",
    "        tokenized_text = tokenizer.tokenize(copy_text)\n",
    "\n",
    "        masked_index = tokenized_text.index('[MASK]')\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # Create the segments tensors.\n",
    "        segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        # Load pre-trained model (weights)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(predictions[0][0][masked_index], dim=-1)\n",
    "        \n",
    "        # All redicted tokens are selected here. len(probs) can be replaced with any number suitable to the use case\n",
    "        top_k_weights, top_k_indicies = torch.topk(probs, len(probs), sorted=True)\n",
    "\n",
    "        output_dict={}\n",
    "        for i, pred_idx in enumerate(top_k_indicies):\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "            token_weight = top_k_weights[i]\n",
    "            if len(original_token)>3:\n",
    "                if textdistance.levenshtein.normalized_similarity(predicted_token,original_token)>0.5:\n",
    "                    output_dict[predicted_token]=float(token_weight)\n",
    "            else:\n",
    "                output_dict[predicted_token]=float(token_weight)\n",
    "        output_dict = dict(list(output_dict.items())[0: top_k]) \n",
    "        print(output_dict)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a very basic implementation but still as we can see it can handle out of vocabulary as well as in vocabulary spelling mistakes. 'diffcult' is an out of vocabulary word and the model could suggest the right word. 'quote' is a spelling mistake where the intended word was 'quite' but this is a in vocabulary word. The model is able to identify that as well. The model can also handle punctuation mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
